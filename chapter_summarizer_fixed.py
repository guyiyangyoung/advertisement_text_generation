import pandas as pd
import json
import ast
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import re
from typing import List, Dict, Optional

class ChapterSummarizerFixed:
    def __init__(self, model_path: str = "Qwen/Qwen2.5-3B-Instruct"):
        """
        åˆå§‹åŒ–ç« èŠ‚æ¦‚æ‹¬å™¨
        """
        self.model_path = model_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        try:
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            
            # ç¡®ä¿æœ‰pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ!")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e
    
    def parse_chapters_data(self, chapters_text) -> Optional[List[Dict]]:
        """
        è§£æchapters_textæ•°æ®ï¼Œæ”¯æŒå¤šç§æ ¼å¼
        """
        print("ğŸ“– æ­£åœ¨è§£æç« èŠ‚æ•°æ®...")
        
        # å¦‚æœå·²ç»æ˜¯listï¼Œç›´æ¥è¿”å›
        if isinstance(chapters_text, list):
            print(f"âœ… æ•°æ®å·²æ˜¯listæ ¼å¼ï¼ŒåŒ…å« {len(chapters_text)} ä¸ªç« èŠ‚")
            return chapters_text
        
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
        if isinstance(chapters_text, str):
            # å°è¯•å¤šç§è§£ææ–¹æ³•
            parsing_methods = [
                ("JSONè§£æ", lambda x: json.loads(x)),
                ("ASTè§£æ", lambda x: ast.literal_eval(x)),
                ("ä¿®å¤åJSONè§£æ", self._fix_and_parse_json),
            ]
            
            for method_name, parse_func in parsing_methods:
                try:
                    print(f"ğŸ”„ å°è¯•ä½¿ç”¨ {method_name}...")
                    result = parse_func(chapters_text)
                    if isinstance(result, list) and len(result) > 0:
                        print(f"âœ… {method_name} æˆåŠŸï¼Œè§£æå‡º {len(result)} ä¸ªç« èŠ‚")
                        return result
                except Exception as e:
                    print(f"âŒ {method_name} å¤±è´¥: {str(e)[:100]}")
                    continue
        
        print("âŒ æ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±è´¥äº†")
        return None
    
    def _fix_and_parse_json(self, text: str) -> List[Dict]:
        """
        å°è¯•ä¿®å¤å¹¶è§£æJSONæ ¼å¼çš„æ–‡æœ¬
        """
        # æ¸…ç†æ–‡æœ¬
        text = text.strip()
        
        # å¦‚æœä¸æ˜¯ä»¥[å¼€å¤´ï¼Œå¯»æ‰¾ç¬¬ä¸€ä¸ª[
        if not text.startswith('['):
            start_idx = text.find('[')
            if start_idx != -1:
                text = text[start_idx:]
        
        # å¦‚æœä¸æ˜¯ä»¥]ç»“å°¾ï¼Œå¯»æ‰¾æœ€åä¸€ä¸ª]
        if not text.endswith(']'):
            end_idx = text.rfind(']')
            if end_idx != -1:
                text = text[:end_idx + 1]
        
        # å°è¯•è§£æ
        return json.loads(text)
    
    def load_and_parse_csv(self, csv_file: str) -> Optional[List[Dict]]:
        """
        åŠ è½½CSVæ–‡ä»¶å¹¶è§£æchapters_textåˆ—
        """
        print(f"ğŸ“ æ­£åœ¨è¯»å–æ–‡ä»¶: {csv_file}")
        
        try:
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(csv_file)
            print(f"ğŸ“Š CSVæ–‡ä»¶åˆ—å: {list(df.columns)}")
            print(f"ğŸ“Š æ•°æ®è¡Œæ•°: {len(df)}")
            
            if 'chapters_text' not in df.columns:
                print("âŒ é”™è¯¯: CSVæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ° 'chapters_text' åˆ—")
                return None
            
            # è·å–chapters_textåˆ—çš„æ•°æ®
            chapters_text = df['chapters_text'].iloc[0]
            print(f"ğŸ“ chapters_text æ•°æ®ç±»å‹: {type(chapters_text)}")
            print(f"ğŸ“ æ•°æ®é•¿åº¦: {len(str(chapters_text))}")
            
            # æ˜¾ç¤ºæ•°æ®çš„å‰200ä¸ªå­—ç¬¦
            preview = str(chapters_text)[:200] + "..." if len(str(chapters_text)) > 200 else str(chapters_text)
            print(f"ğŸ“ æ•°æ®é¢„è§ˆ: {preview}")
            
            # è§£æç« èŠ‚æ•°æ®
            chapters_data = self.parse_chapters_data(chapters_text)
            
            if chapters_data:
                # æ˜¾ç¤ºå‰å‡ ä¸ªç« èŠ‚çš„ä¿¡æ¯
                print(f"\nğŸ“š æˆåŠŸè§£æå‡º {len(chapters_data)} ä¸ªç« èŠ‚:")
                for i, chapter in enumerate(chapters_data[:3]):
                    title = chapter.get('title', f'ç¬¬{i+1}ç« ')
                    content_length = len(chapter.get('content', ''))
                    print(f"  ğŸ“– ç« èŠ‚ {i+1}: {title} (å†…å®¹é•¿åº¦: {content_length} å­—)")
                
                if len(chapters_data) > 3:
                    print(f"  ... è¿˜æœ‰ {len(chapters_data) - 3} ä¸ªç« èŠ‚")
            
            return chapters_data
            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶è¯»å–é”™è¯¯: {e}")
            return None
    
    def clean_text(self, text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬å†…å®¹
        """
        if not text:
            return ""
        
        # ç§»é™¤ç« èŠ‚ç¼–å·ï¼ˆå¦‚æœåœ¨å†…å®¹å¼€å¤´ï¼‰
        text = re.sub(r'^\d+\s+', '', text.strip())
        
        # ç§»é™¤æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼
        text = re.sub(r'\n+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤å¼•å·å’Œå¯¹è¯æ ‡è®°
        text = re.sub(r'^["""]', '', text)
        text = re.sub(r'["""]$', '', text)
        
        # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œå¸¸ç”¨æ ‡ç‚¹
        text = re.sub(r'[^\u4e00-\u9fff\u0030-\u0039\u0041-\u005a\u0061-\u007a\sï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€Šã€‹ã€ã€‘ã€]', '', text)
        
        return text.strip()
    
    def create_summary_prompt(self, title: str, content: str) -> str:
        """
        åˆ›å»ºæ¦‚æ‹¬æç¤ºè¯
        """
        return f"""è¯·å°†ä»¥ä¸‹å°è¯´ç« èŠ‚æ¦‚æ‹¬ä¸ºçº¦200å­—çš„ç²¾ç‚¼æ–‡å­—ã€‚

è¦æ±‚ï¼š
1. æå–å…³é”®æƒ…èŠ‚å’Œäººç‰©è¡Œä¸º
2. ä¿æŒæ•…äº‹çš„è¿è´¯æ€§å’Œé€»è¾‘æ€§
3. è¯­è¨€ç®€æ´æµç•…ï¼Œçªå‡ºé‡ç‚¹
4. å­—æ•°æ§åˆ¶åœ¨180-220å­—ä¹‹é—´

ç« èŠ‚æ ‡é¢˜ï¼š{title}

ç« èŠ‚å†…å®¹ï¼š
{content}

è¯·å¼€å§‹æ¦‚æ‹¬ï¼š"""
    
    def summarize_chapter(self, title: str, content: str, max_input_length: int = 1800) -> str:
        """
        ä½¿ç”¨æ¨¡å‹å¯¹å•ä¸ªç« èŠ‚è¿›è¡Œæ¦‚æ‹¬
        """
        # æ¸…ç†å†…å®¹
        cleaned_content = self.clean_text(content)
        
        # æ§åˆ¶è¾“å…¥é•¿åº¦
        if len(cleaned_content) > max_input_length:
            cleaned_content = cleaned_content[:max_input_length] + "..."
        
        # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œç›´æ¥è¿”å›
        if len(cleaned_content) < 20:
            return "[å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•æ¦‚æ‹¬]"
        
        # åˆ›å»ºæç¤ºè¯
        prompt = self.create_summary_prompt(title, cleaned_content)
        
        try:
            # æ„å»ºå¯¹è¯
            messages = [
                {
                    "role": "system", 
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡å­¦ä½œå“æ¦‚æ‹¬ä¸“å®¶ï¼Œæ“…é•¿æå–æ•…äº‹çš„æ ¸å¿ƒæƒ…èŠ‚å’Œå…³é”®ä¿¡æ¯ï¼Œç”¨ç®€æ´çš„è¯­è¨€è¿›è¡Œç²¾å‡†æ¦‚æ‹¬ã€‚"
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ]
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                text, 
                return_tensors="pt", 
                truncation=True, 
                max_length=2048
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            if torch.cuda.is_available():
                inputs = inputs.to(self.device)
            
            # ç”Ÿæˆæ¦‚æ‹¬
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=280,
                    temperature=0.3,
                    top_p=0.85,
                    do_sample=True,
                    repetition_penalty=1.15,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç è¾“å‡º
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            )
            
            # æ¸…ç†è¾“å‡º
            summary = generated_text.strip()
            
            # ç§»é™¤å¯èƒ½çš„é‡å¤å†…å®¹æˆ–æ— å…³æ–‡æœ¬
            if "æ¦‚æ‹¬ï¼š" in summary:
                summary = summary.split("æ¦‚æ‹¬ï¼š")[-1].strip()
            
            return summary if summary else "[ç”Ÿæˆå¤±è´¥]"
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ¦‚æ‹¬æ—¶å‡ºé”™: {e}")
            return f"[ç”Ÿæˆå¤±è´¥: {str(e)}]"
    
    def process_all_chapters(self, chapters_data: List[Dict]) -> List[Dict]:
        """
        å¤„ç†æ‰€æœ‰ç« èŠ‚
        """
        results = []
        total_chapters = len(chapters_data)
        
        print(f"\nğŸ”„ å¼€å§‹å¤„ç† {total_chapters} ä¸ªç« èŠ‚...")
        print("=" * 60)
        
        for i, chapter in enumerate(chapters_data, 1):
            title = chapter.get('title', f'ç¬¬{i}ç« ')
            content = chapter.get('content', '')
            
            print(f"ğŸ“– [{i:2d}/{total_chapters}] æ­£åœ¨å¤„ç†: {title}")
            
            if not content.strip():
                summary = "[ç« èŠ‚æ— å†…å®¹]"
                print(f"âš ï¸  è­¦å‘Š: ç« èŠ‚å†…å®¹ä¸ºç©º")
            else:
                print(f"ğŸ“ åŸæ–‡é•¿åº¦: {len(content)} å­—")
                summary = self.summarize_chapter(title, content)
                print(f"âœ… æ¦‚æ‹¬å®Œæˆ: {len(summary)} å­—")
            
            results.append({
                'chapter_num': i,
                'title': title,
                'summary': summary,
                'original_length': len(content),
                'summary_length': len(summary)
            })
            
            print("-" * 40)
        
        return results
    
    def create_integrated_summary(self, chapter_summaries: List[Dict]) -> str:
        """
        åˆ›å»ºæ•´åˆçš„æ•…äº‹æ¦‚æ‹¬
        """
        print("\nğŸ”„ æ­£åœ¨ç”Ÿæˆæ•´åˆæ¦‚æ‹¬...")
        
        # æ”¶é›†æœ‰æ•ˆçš„æ¦‚æ‹¬
        valid_summaries = []
        for chapter in chapter_summaries:
            summary = chapter['summary']
            if not summary.startswith('[') and len(summary.strip()) > 10:
                valid_summaries.append(summary)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_chapters = len(chapter_summaries)
        total_original_length = sum(ch['original_length'] for ch in chapter_summaries)
        total_summary_length = sum(ch['summary_length'] for ch in chapter_summaries)
        
        # æ„å»ºæœ€ç»ˆæ¦‚æ‹¬
        integrated_story = " ".join(valid_summaries)
        
        final_text = f"""ã€Šå½“å®¶ä¸»æ¯ï¼Œæˆ‘åœ¨ç°ä»£æ•´æ²»é¡¶çº§è±ªé—¨ã€‹å‰{total_chapters}ç« æ¦‚æ‹¬

ã€æ•…äº‹æ¢—æ¦‚ã€‘
{integrated_story}

ã€ç»Ÿè®¡ä¿¡æ¯ã€‘
â€¢ æ€»ç« èŠ‚æ•°ï¼š{total_chapters} ç« 
â€¢ åŸæ–‡æ€»å­—æ•°ï¼š{total_original_length:,} å­—
â€¢ æ¦‚æ‹¬æ€»å­—æ•°ï¼š{total_summary_length:,} å­—
â€¢ æœ‰æ•ˆæ¦‚æ‹¬ç« èŠ‚ï¼š{len(valid_summaries)} ç« 
â€¢ å†…å®¹å‹ç¼©æ¯”ï¼š{(total_summary_length/total_original_length*100):.1f}%

ã€å¤„ç†å®Œæˆæ—¶é—´ã€‘
{pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}"""
        
        return final_text
    
    def save_results(self, chapter_summaries: List[Dict], final_summary: str, output_file: str = "novel_summary_result.txt"):
        """
        ä¿å­˜å¤„ç†ç»“æœ
        """
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœåˆ°: {output_file}")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            # å†™å…¥æ ‡é¢˜
            f.write("=" * 80 + "\n")
            f.write("å°è¯´ç« èŠ‚æ¦‚æ‹¬ç»“æœ\n")
            f.write("=" * 80 + "\n\n")
            
            # å†™å…¥ç« èŠ‚è¯¦ç»†æ¦‚æ‹¬
            f.write("ã€ç« èŠ‚è¯¦ç»†æ¦‚æ‹¬ã€‘\n")
            f.write("-" * 60 + "\n\n")
            
            for chapter in chapter_summaries:
                f.write(f"ç« èŠ‚ç¼–å·ï¼š{chapter['chapter_num']}\n")
                f.write(f"ç« èŠ‚æ ‡é¢˜ï¼š{chapter['title']}\n")
                f.write(f"åŸæ–‡é•¿åº¦ï¼š{chapter['original_length']} å­—\n")
                f.write(f"æ¦‚æ‹¬é•¿åº¦ï¼š{chapter['summary_length']} å­—\n")
                f.write(f"æ¦‚æ‹¬å†…å®¹ï¼š{chapter['summary']}\n")
                f.write("\n" + "â€”" * 50 + "\n\n")
            
            # å†™å…¥æ•´åˆæ¦‚æ‹¬
            f.write("\n" + "=" * 80 + "\n")
            f.write("ã€æ•´åˆæ•…äº‹æ¦‚æ‹¬ã€‘\n")
            f.write("=" * 80 + "\n\n")
            f.write(final_summary)
        
        print(f"âœ… ç»“æœå·²æˆåŠŸä¿å­˜åˆ°: {output_file}")

def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸ¯ å°è¯´ç« èŠ‚æ¦‚æ‹¬å·¥å…·")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–æ¦‚æ‹¬å™¨
        summarizer = ChapterSummarizerFixed()
        
        # åŠ è½½å’Œè§£ææ•°æ®
        chapters_data = summarizer.load_and_parse_csv("test.csv")
        if not chapters_data:
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return
        
        # å¤„ç†æ‰€æœ‰ç« èŠ‚
        chapter_summaries = summarizer.process_all_chapters(chapters_data)
        
        # ç”Ÿæˆæ•´åˆæ¦‚æ‹¬
        final_summary = summarizer.create_integrated_summary(chapter_summaries)
        
        # ä¿å­˜ç»“æœ
        summarizer.save_results(chapter_summaries, final_summary)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print("\n" + "=" * 80)
        print("ğŸ‰ å¤„ç†å®Œæˆï¼æœ€ç»ˆæ¦‚æ‹¬ç»“æœï¼š")
        print("=" * 80)
        print(final_summary)
        
        print("\nâœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼è¯·æŸ¥çœ‹è¾“å‡ºæ–‡ä»¶ 'novel_summary_result.txt'")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()