import pandas as pd
import json
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import re
import os
from typing import List, Dict, Optional

class Qwen3ChapterSummarizer:
    def __init__(self, model_path: str = "Qwen/Qwen2.5-3B-Instruct"):
        """
        åˆå§‹åŒ–Qwen3ç« èŠ‚æ¦‚æ‹¬å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼Œå¯ä»¥æ˜¯ï¼š
                - "Qwen/Qwen2.5-3B-Instruct" (å…¬å¼€å¯ç”¨)
                - "/path/to/local/qwen3-8b" (æœ¬åœ°æ¨¡å‹è·¯å¾„)
                - "Qwen/Qwen2.5-7B-Instruct" (æ›´å¤§çš„å…¬å¼€æ¨¡å‹)
        """
        self.model_path = model_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        try:
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                padding_side="left"
            )
            
            # ç¡®ä¿æœ‰pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            print("æ¨¡å‹åŠ è½½å®Œæˆ!")
            
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("å°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å‹...")
            self._load_fallback_model()
    
    def _load_fallback_model(self):
        """åŠ è½½å¤‡ç”¨æ¨¡å‹"""
        fallback_model = "Qwen/Qwen2.5-3B-Instruct"
        print(f"åŠ è½½å¤‡ç”¨æ¨¡å‹: {fallback_model}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(fallback_model)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        self.model = AutoModelForCausalLM.from_pretrained(
            fallback_model,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
    
    def load_and_parse_csv(self, csv_file: str) -> Optional[List[Dict]]:
        """
        åŠ è½½CSVæ–‡ä»¶å¹¶è§£æchapters_textåˆ—
        """
        print(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {csv_file}")
        
        try:
            df = pd.read_csv(csv_file)
            print(f"CSVæ–‡ä»¶åˆ—å: {list(df.columns)}")
            
            if 'chapters_text' not in df.columns:
                print("é”™è¯¯: CSVæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ° 'chapters_text' åˆ—")
                return None
            
            # è·å–chapters_textåˆ—çš„æ•°æ® (å‡è®¾åªæœ‰ä¸€è¡Œæ•°æ®)
            chapters_text = df['chapters_text'].iloc[0]
            
            # è§£æJSONæ•°æ®
            chapters_data = json.loads(chapters_text)
            print(f"æˆåŠŸè§£æå‡º {len(chapters_data)} ä¸ªç« èŠ‚")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªç« èŠ‚çš„æ ‡é¢˜
            for i, chapter in enumerate(chapters_data[:3]):
                title = chapter.get('title', f'ç¬¬{i+1}ç« ')
                print(f"  ç« èŠ‚ {i+1}: {title}")
            
            return chapters_data
            
        except json.JSONDecodeError as e:
            print(f"JSONè§£æé”™è¯¯: {e}")
            print("å°è¯•ä¿®å¤JSONæ ¼å¼...")
            return self._try_fix_json(chapters_text)
        except Exception as e:
            print(f"æ–‡ä»¶è¯»å–é”™è¯¯: {e}")
            return None
    
    def _try_fix_json(self, text: str) -> Optional[List[Dict]]:
        """å°è¯•ä¿®å¤JSONæ ¼å¼"""
        try:
            # ç§»é™¤å¯èƒ½çš„å‰åç¼€
            text = text.strip()
            if not text.startswith('['):
                # å¯»æ‰¾ç¬¬ä¸€ä¸ª [
                start = text.find('[')
                if start != -1:
                    text = text[start:]
            
            if not text.endswith(']'):
                # å¯»æ‰¾æœ€åä¸€ä¸ª ]
                end = text.rfind(']')
                if end != -1:
                    text = text[:end+1]
            
            return json.loads(text)
        except:
            print("JSONä¿®å¤å¤±è´¥")
            return None
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""
        
        # ç§»é™¤æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼
        text = re.sub(r'\n+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤ç‰¹æ®Šæ§åˆ¶å­—ç¬¦ï¼Œä½†ä¿ç•™ä¸­æ–‡æ ‡ç‚¹
        text = re.sub(r'[^\u4e00-\u9fff\u0030-\u0039\u0041-\u005a\u0061-\u007a\sï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€Šã€‹ã€ã€‘ã€]', '', text)
        
        return text.strip()
    
    def create_summary_prompt(self, title: str, content: str) -> str:
        """åˆ›å»ºæ¦‚æ‹¬æç¤ºè¯"""
        return f"""è¯·å°†ä»¥ä¸‹å°è¯´ç« èŠ‚æ¦‚æ‹¬ä¸º200å­—å·¦å³çš„ç²¾ç‚¼æ–‡å­—ã€‚è¦æ±‚ï¼š
1. çªå‡ºå…³é”®æƒ…èŠ‚å’Œäººç‰©åŠ¨ä½œ
2. ä¿æŒæ•…äº‹è¿è´¯æ€§
3. è¯­è¨€ç®€æ´æ˜äº†
4. å­—æ•°æ§åˆ¶åœ¨180-220å­—ä¹‹é—´

ç« èŠ‚æ ‡é¢˜ï¼š{title}

ç« èŠ‚å†…å®¹ï¼š
{content}

æ¦‚æ‹¬ï¼š"""
    
    def summarize_chapter(self, title: str, content: str, max_length: int = 2000) -> str:
        """
        ä½¿ç”¨æ¨¡å‹å¯¹å•ä¸ªç« èŠ‚è¿›è¡Œæ¦‚æ‹¬
        """
        # æ¸…ç†å†…å®¹
        cleaned_content = self.clean_text(content)
        
        # é™åˆ¶è¾“å…¥é•¿åº¦
        if len(cleaned_content) > max_length:
            cleaned_content = cleaned_content[:max_length] + "..."
        
        # åˆ›å»ºæç¤ºè¯
        prompt = self.create_summary_prompt(title, cleaned_content)
        
        try:
            # æ„å»ºå¯¹è¯
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°è¯´å†…å®¹æ¦‚æ‹¬åŠ©æ‰‹ï¼Œæ“…é•¿æå–å…³é”®æƒ…èŠ‚ï¼Œç”¨ç®€æ´çš„è¯­è¨€æ¦‚æ‹¬ç« èŠ‚å†…å®¹ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # ç¼–ç 
            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=2048)
            if torch.cuda.is_available():
                inputs = inputs.to(self.device)
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=250,
                    temperature=0.3,  # é™ä½éšæœºæ€§ï¼Œä½¿è¾“å‡ºæ›´ç¨³å®š
                    top_p=0.8,
                    do_sample=True,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç 
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            )
            
            return generated_text.strip()
            
        except Exception as e:
            print(f"ç”Ÿæˆæ¦‚æ‹¬æ—¶å‡ºé”™: {e}")
            return f"[ç”Ÿæˆå¤±è´¥] {str(e)}"
    
    def process_all_chapters(self, chapters_data: List[Dict]) -> List[Dict]:
        """å¤„ç†æ‰€æœ‰ç« èŠ‚"""
        results = []
        
        print(f"å¼€å§‹å¤„ç† {len(chapters_data)} ä¸ªç« èŠ‚...")
        
        for i, chapter in enumerate(chapters_data, 1):
            title = chapter.get('title', f'ç¬¬{i}ç« ')
            content = chapter.get('content', '')
            
            print(f"[{i}/{len(chapters_data)}] æ­£åœ¨å¤„ç†: {title}")
            
            if not content:
                summary = "[æ— å†…å®¹]"
            else:
                summary = self.summarize_chapter(title, content)
            
            results.append({
                'chapter_num': i,
                'title': title,
                'summary': summary,
                'original_length': len(content)
            })
            
            print(f"å®Œæˆæ¦‚æ‹¬ (åŸæ–‡{len(content)}å­— -> æ¦‚æ‹¬{len(summary)}å­—)")
        
        return results
    
    def create_final_summary(self, chapter_summaries: List[Dict]) -> str:
        """åˆ›å»ºæœ€ç»ˆçš„å®Œæ•´æ¦‚æ‹¬"""
        print("æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ¦‚æ‹¬...")
        
        # æ„å»ºå®Œæ•´çš„æ•…äº‹æ¦‚æ‹¬
        story_parts = []
        
        for chapter in chapter_summaries:
            if not chapter['summary'].startswith('['):  # æ’é™¤é”™è¯¯ä¿¡æ¯
                story_parts.append(chapter['summary'])
        
        # åˆå¹¶ä¸ºè¿è´¯çš„æ•…äº‹
        final_story = " ".join(story_parts)
        
        # æ·»åŠ æ ‡é¢˜å’Œç»Ÿè®¡ä¿¡æ¯
        total_chapters = len(chapter_summaries)
        total_original_length = sum(ch['original_length'] for ch in chapter_summaries)
        
        final_text = f"""ã€Šå½“å®¶ä¸»æ¯ï¼Œæˆ‘åœ¨ç°ä»£æ•´æ²»é¡¶çº§è±ªé—¨ã€‹å‰{total_chapters}ç« æ¦‚æ‹¬

ã€æ•…äº‹æ¢—æ¦‚ã€‘
{final_story}

ã€ç»Ÿè®¡ä¿¡æ¯ã€‘
- æ€»ç« èŠ‚æ•°ï¼š{total_chapters}ç« 
- åŸæ–‡æ€»å­—æ•°ï¼š{total_original_length:,}å­—
- æ¦‚æ‹¬æ€»å­—æ•°ï¼š{len(final_story):,}å­—
- å‹ç¼©æ¯”ï¼š{len(final_story)/total_original_length*100:.1f}%"""
        
        return final_text
    
    def save_results(self, chapter_summaries: List[Dict], final_summary: str, output_file: str = "novel_summary.txt"):
        """ä¿å­˜ç»“æœ"""
        with open(output_file, 'w', encoding='utf-8') as f:
            # å†™å…¥è¯¦ç»†çš„ç« èŠ‚æ¦‚æ‹¬
            f.write("=" * 60 + "\n")
            f.write("ç« èŠ‚è¯¦ç»†æ¦‚æ‹¬\n")
            f.write("=" * 60 + "\n\n")
            
            for chapter in chapter_summaries:
                f.write(f"ã€{chapter['title']}ã€‘\n")
                f.write(f"åŸæ–‡é•¿åº¦ï¼š{chapter['original_length']}å­—\n")
                f.write(f"æ¦‚æ‹¬ï¼š{chapter['summary']}\n")
                f.write("-" * 40 + "\n\n")
            
            # å†™å…¥æœ€ç»ˆæ¦‚æ‹¬
            f.write("\n" + "=" * 60 + "\n")
            f.write("å®Œæ•´æ•…äº‹æ¦‚æ‹¬\n")
            f.write("=" * 60 + "\n\n")
            f.write(final_summary)
        
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    # å¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹æ¨¡å‹è·¯å¾„
    # å¦‚æœæ‚¨æœ‰Qwen3-8Bçš„è®¿é—®æƒé™ï¼Œè¯·ä¿®æ”¹ä¸‹é¢çš„è·¯å¾„
    model_options = [
        "Qwen/Qwen2.5-3B-Instruct",  # é»˜è®¤é€‰é¡¹
        "Qwen/Qwen2.5-7B-Instruct",  # æ›´å¤§çš„æ¨¡å‹
        # "/path/to/your/qwen3-8b",   # æœ¬åœ°Qwen3-8Bè·¯å¾„
    ]
    
    # åˆå§‹åŒ–æ¦‚æ‹¬å™¨
    summarizer = Qwen3ChapterSummarizer(model_options[0])
    
    # åŠ è½½å’Œè§£ææ•°æ®
    chapters_data = summarizer.load_and_parse_csv("test.csv")
    if not chapters_data:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    # å¤„ç†æ‰€æœ‰ç« èŠ‚
    chapter_summaries = summarizer.process_all_chapters(chapters_data)
    
    # ç”Ÿæˆæœ€ç»ˆæ¦‚æ‹¬
    final_summary = summarizer.create_final_summary(chapter_summaries)
    
    # ä¿å­˜ç»“æœ
    summarizer.save_results(chapter_summaries, final_summary)
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“– æœ€ç»ˆæ¦‚æ‹¬ç»“æœ")
    print("=" * 60)
    print(final_summary)
    print("\nâœ… å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()